{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNyf4EpxaJySJ9zoBvHiY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAHIL9581/w2w/blob/main/W2W_Pipeline_Orchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izMaAxWwY3c1",
        "outputId": "091a7314-55ad-4e64-f07e-523d875bc580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Creating a temporary project workspace at: /content/W2W_Pipeline_Local\n",
            "/content/W2W_Pipeline_Local\n",
            "--> Successfully changed directory to: /content/W2W_Pipeline_Local\n",
            "--> Installing all necessary Python libraries...\n",
            "--> Library installation complete.\n",
            "--> Creating all project source files...\n",
            "\n",
            "✅ All project files created successfully!\n"
          ]
        }
      ],
      "source": [
        "# --- 1. SETUP THE TEMPORARY ENVIRONMENT AND WORKSPACE ---\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "# We will work inside Colab's temporary storage.\n",
        "PROJECT_PATH = \"/content/W2W_Pipeline_Local\"\n",
        "print(f\"--> Creating a temporary project workspace at: {PROJECT_PATH}\")\n",
        "\n",
        "os.makedirs(f\"{PROJECT_PATH}/src\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/data/raw_las_files\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/artifacts\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/trained_models/autoencoder\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/trained_models/boundary_detector\", exist_ok=True)\n",
        "\n",
        "%cd {PROJECT_PATH}\n",
        "print(f\"--> Successfully changed directory to: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# --- 2. INSTALL ALL REQUIRED LIBRARIES ---\n",
        "print(\"--> Installing all necessary Python libraries...\")\n",
        "!pip install \"ray[train,tune]>=2.9.0\" pandas numpy torch scikit-learn pyyaml mlflow scipy joblib warmup_scheduler lasio matplotlib pyngrok -q\n",
        "print(\"--> Library installation complete.\")\n",
        "\n",
        "\n",
        "# --- 3. CREATE ALL PROJECT SCRIPTS AND CONFIGURATION ---\n",
        "print(\"--> Creating all project source files...\")\n",
        "\n",
        "# --- config.yaml ---\n",
        "with open(\"config.yaml\", \"w\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "    run_data_preparation: true\n",
        "    run_pretraining: true\n",
        "    run_finetuning: true\n",
        "    run_inference: true\n",
        "    paths:\n",
        "      raw_las_folder: \"data/raw_las_files/\"\n",
        "      processed_csv_path: \"data/train.csv\"\n",
        "      label_encoder_path: \"artifacts/label_encoder.json\"\n",
        "      std_scaler_path: \"artifacts/StandardScaler.bin\"\n",
        "      pretrained_encoder_path: \"trained_models/autoencoder/best_autoencoder.pt\"\n",
        "      final_model_path: \"trained_models/boundary_detector/final_model.pt\"\n",
        "    mlflow:\n",
        "      experiment_name: \"W2W_Matcher_Pipeline\"\n",
        "    pretraining:\n",
        "      epochs: 25\n",
        "      search_space:\n",
        "        in_channels: 13\n",
        "        optimizer: [\"RMSprop\", \"AdamW\", \"Adam\"]\n",
        "        lr: [0.001, 0.0001]\n",
        "        act_name: [\"prelu\", \"relu\"]\n",
        "        batch_size: [16, 32]\n",
        "    finetuning:\n",
        "      learning_rate: 0.0001\n",
        "      batch_size: 16\n",
        "      epochs: 100\n",
        "      model_params: {patch_height: 700, in_channels: 13, act_name: \"prelu\", project_in_features: 2048, hidden_dim: 256, num_queries: 100, num_heads: 8, dropout: 0.1, expansion_factor: 4, num_transformers: 6, output_size: 3}\n",
        "      matcher_costs: {set_cost_class: 1, set_cost_bbox: 5}\n",
        "      loss_weights: {loss_matching: 1.0, loss_unmatching: 0.5, loss_height_constraint: 0.5}\n",
        "    inference:\n",
        "      reference_well: \"WELL_NAME_A\"\n",
        "      well_of_interest: \"WELL_NAME_B\"\n",
        "      correlation_threshold: 0.7\n",
        "    \"\"\"))\n",
        "\n",
        "# --- ALL OTHER SCRIPTS (with corrected pretrain_autoencoder.py) ---\n",
        "with open(\"src/__init__.py\", \"w\") as f: f.write(\"# Makes this a package\\n\")\n",
        "with open(\"src/prepare_data.py\", \"w\") as f: f.write(\"import pandas as pd,numpy as np,lasio,os,json\\nfrom sklearn.preprocessing import StandardScaler\\nfrom joblib import dump\\ndef run_data_preparation(config):\\n    print(\\\"--- LAUNCHING PIPELINE 0: DATA PREPARATION ---\\\")\\n    paths = config['paths']\\n    search_folder = paths['raw_las_folder']\\n    items_in_folder = os.listdir(search_folder)\\n    if len(items_in_folder) == 1 and os.path.isdir(os.path.join(search_folder, items_in_folder[0])):\\n        print(f\\\"--> Found single sub-folder '{items_in_folder[0]}'. Adjusting search path.\\\")\\n        search_folder = os.path.join(search_folder, items_in_folder[0])\\n    all_wells_df, las_files_found = [], []\\n    print(f\\\"--> Searching for .las files in '{search_folder}'...\\\")\\n    for root, dirs, files in os.walk(search_folder):\\n        for file in files:\\n            if file.lower().endswith('.las'): las_files_found.append(os.path.join(root, file))\\n    if not las_files_found: raise FileNotFoundError(f\\\"No .las files found in '{search_folder}'. Check your ZIP.\\\")\\n    print(f\\\"--> Found {len(las_files_found)} .las files. Reading now...\\\")\\n    for filepath in las_files_found:\\n        try:\\n            las = lasio.read(filepath); df = las.df().reset_index()\\n            df['WELL'] = las.well.WELL.value if las.well.WELL.value else os.path.splitext(os.path.basename(filepath))[0]; df['GROUP'] = 'UNKNOWN'\\n            for param in las.params:\\n                if 'GROUP' in param.mnemonic: df['GROUP'] = param.value; break\\n            all_wells_df.append(df)\\n        except Exception as e: print(f\\\"    - Could not read {filepath}: {e}\\\")\\n    master_df = pd.concat(all_wells_df, ignore_index=True)\\n    if 'DEPT' in master_df.columns: master_df.rename(columns={'DEPT':'DEPTH_MD'}, inplace=True)\\n    master_df.to_csv(paths['processed_csv_path'], index=False, sep=';'); print(f\\\"--> Saved combined data to '{paths['processed_csv_path']}'\\\")\\n    label_encoder={str(g):i for i, g in enumerate(master_df['GROUP'].unique())}\\n    with open(paths['label_encoder_path'], 'w') as f: json.dump(label_encoder, f, indent=4)\\n    print(f\\\"--> Saved label encoder to '{paths['label_encoder_path']}'\\\")\\n    numeric_df = master_df.drop(columns=['WELL', 'GROUP', 'DEPTH_MD'], errors='ignore')\\n    scaler = StandardScaler(); scaler.fit(numeric_df.fillna(0)); dump(scaler, paths['std_scaler_path'])\\n    print(f\\\"--> Saved StandardScaler to '{paths['std_scaler_path']}'\\\"); print(\\\"✅ Data Preparation complete.\\\")\\n\")\n",
        "with open(\"src/utils.py\", \"w\") as f: f.write(\"import torch\\ndef collate_fn(batch):images,t=zip(*batch);return torch.stack(images),list(t)\\n\")\n",
        "with open(\"src/dataset_pretrain.py\", \"w\") as f: f.write(\"import numpy as np,torch,pandas as pd\\nfrom torch.utils import data\\nfrom joblib import load\\nclass AutoencoderDataset(data.Dataset):\\n    def __init__(self,c):\\n        p=c['paths'];df=pd.read_csv(p['processed_csv_path'],delimiter=';')\\n        cols_drop=['WELL','GROUP','DEPTH_MD']\\n        df.drop(columns=cols_drop,inplace=True,errors='ignore');df.fillna(0,inplace=True)\\n        scaler=load(p['std_scaler_path']);self.data=scaler.transform(df).astype(np.float32)\\n    def __len__(self):return len(self.data)\\n    def __getitem__(self,i):s=self.data[i];return torch.from_numpy(s),torch.from_numpy(s)\\n\")\n",
        "with open(\"src/dataset_finetune.py\", \"w\") as f: f.write(\"import json,numpy as np,torch,pandas as pd\\nfrom torch.utils import data\\nfrom joblib import load\\nclass BoundaryDataset(data.Dataset):\\n    def __init__(self,c,seed=None):\\n        self.p=c['finetuning']['model_params'];self.d=c['paths'];self.s=seed if seed else np.random.randint(2**32-1)\\n        x,y=self.get_Xy();self.x=x;self.gt=y\\n    def load_df(self,p,d=';'):return pd.read_csv(p,delimiter=d)\\n    def get_rand_well(self,d,s):np.random.seed(s);names=list(d.WELL.unique());idx=np.random.randint(0,len(names));return d[d['WELL']==names[idx]].copy()\\n    def get_gt_b(self,y_l):\\n        gts=[];\\n        for n,y in enumerate(y_l):\\n            gt,c={},0;k=[i+1 for i in range(len(y)-1) if not y[i]==y[i+1]];k.insert(0,0);gp=[y[idx] for idx in k];top=k.copy();k.append(len(y));h=[e1-e2 for(e1,e2) in zip(k[1:],k[:-1])]\\n            for t,h_val,g in zip(top,h,gp):gt[c]={'Group':int(g),'Top':int(t),'Height':int(h_val)};c+=1\\n            gts.append(gt)\\n        return gts\\n    def get_Xy(self):\\n        d=self.load_df(self.d['processed_csv_path']);w=self.get_rand_well(d,self.s)\\n        with open(self.d['label_encoder_path']) as f:le=json.load(f)\\n        w.loc[:,'GROUP']=w['GROUP'].astype(str).map(le).bfill().ffill()\\n        lbl=w['GROUP'].copy()\\n        cols_drop=['WELL','GROUP','DEPTH_MD']\\n        w_numeric=w.drop(columns=cols_drop,errors='ignore');w_numeric.fillna(0,inplace=True)\\n        scaler=load(self.d['std_scaler_path']);s_d=scaler.transform(w_numeric)\\n        ph=self.p['patch_height'];idx=list(range(0,s_d.shape[0],ph))\\n        x=np.asarray([s_d[i:i+ph] for i in idx if s_d[i:i+ph].shape[0]==ph]).astype(np.float32)\\n        y=np.asarray([lbl.values[i:i+ph] for i in idx if lbl.values[i:i+ph].shape[0]==ph])\\n        return x,self.get_gt_b(y)\\n    def __len__(self):return len(self.x)\\n    def __getitem__(self,idx):\\n        img=np.expand_dims(self.x[idx],0);data=self.gt[idx];lbl,top,h=[],[],[]\\n        for i in data:top.append(data[i]['Top']/self.p['patch_height']);h.append(data[i]['Height']/self.p['patch_height']);lbl.append(1)\\n        tgt={};tgt['labels']=torch.tensor(lbl,dtype=torch.long);t,h_v=torch.tensor(top,dtype=torch.float32).view(-1,1),torch.tensor(h,dtype=torch.float32).view(-1,1)\\n        tgt['loc_info']=torch.hstack((t,h_v));return torch.from_numpy(img),tgt\\n\")\n",
        "with open(\"src/model.py\", \"w\") as f: f.write(\"import torch,torch.nn as nn\\ndef get_activation(name): return nn.PReLU() if name=='prelu' else nn.ReLU() if name=='relu' else nn.GELU()\\nclass Project(nn.Module):\\n    def __init__(self,i,o): super().__init__(); self.l=nn.Linear(i,o)\\n    def forward(self,x): return self.l(x.flatten(1))\\nclass Query(nn.Module):\\n    def __init__(self,s,d): super().__init__(); self.q=nn.Parameter(torch.randn(1,s,d))\\n    def forward(self,x): return self.q.repeat(x.shape[0],1,1)\\nclass Transformer(nn.Module):\\n    def __init__(self,i,n,d,e,a): super().__init__(); self.t=nn.TransformerEncoderLayer(d_model=i,nhead=n,dropout=d,batch_first=True)\\n    def forward(self,q,c): return self.t(q)\\nclass Block(nn.Module):\\n    def __init__(self,i,o,s=2,k=3,a='prelu'):\\n        super().__init__(); p=k//2; self.act=get_activation(a); self.b=nn.Sequential(nn.Conv2d(i,o,k,s,p),nn.BatchNorm2d(o),self.act,nn.Conv2d(o,o,k,1,p),nn.BatchNorm2d(o),self.act)\\n    def forward(self,x): return self.b(x)\\nclass UNet(nn.Module):\\n    def __init__(self,in_channels=13,activation='prelu'):\\n        super().__init__();self.act=get_activation(activation);self.start=nn.Sequential(nn.Conv2d(in_channels,32,3,1,1),nn.BatchNorm2d(32),self.act);self.e1=Block(32,64,2,a=activation);self.e2=Block(64,128,2,a=activation);self.e3=Block(128,256,2,a=activation);self.mid=nn.Sequential(nn.Conv2d(256,512,2),nn.BatchNorm2d(512),self.act);self.uc3=nn.ConvTranspose2d(512,256,2,2);self.d3=Block(512,256,1,a=activation);self.uc2=nn.ConvTranspose2d(256,128,2,2);self.d2=Block(256,128,1,a=activation);self.uc1=nn.ConvTranspose2d(128,64,2,2);self.d1=Block(128,64,1,a=activation);self.out=nn.Conv2d(64,in_channels,1)\\n    def forward(self,x):\\n        x=x.unsqueeze(-1).unsqueeze(-1);x1=self.e1(self.start(x));x2=self.e2(x1);x3=self.e3(x2);m=self.mid(x3);u3=self.d3(torch.cat((self.uc3(m,output_size=x3.size()),x3),1));u2=self.d2(torch.cat((self.uc2(u3,output_size=x2.size()),x2),1));u1=self.d1(torch.cat((self.uc1(u2,output_size=x1.size()),x1),1));return self.out(u1).squeeze(-1).squeeze(-1)\\nclass UNetEncoder(nn.Module):\\n    def __init__(self,in_channels=13,activation='prelu'):\\n        super().__init__();self.act=get_activation(activation);self.start=nn.Sequential(nn.Conv2d(in_channels,32,3,1,1),nn.BatchNorm2d(32),self.act);self.e1=Block(32,64,2,a=activation);self.e2=Block(64,128,2,a=activation);self.e3=Block(128,256,2,a=activation);self.mid=nn.Sequential(nn.Conv2d(256,512,2),nn.BatchNorm2d(512),self.act)\\n    def forward(self,x):x=x.unsqueeze(-1);x=x.permute(0,2,1);x=x.unsqueeze(-1);x1=self.e1(self.start(x));x2=self.e2(x1);x3=self.e3(x2);m=self.mid(x3);return m\\nclass W2WTransformerModel(nn.Module):\\n    def __init__(self,c):\\n        super().__init__();p=c['finetuning']['model_params'];self.encoder=UNetEncoder(p['in_channels'],p['act_name']);self.project=Project(p['project_in_features'],p['hidden_dim']);self.query=Query(p['num_queries'],p['hidden_dim']);self.transformers=nn.ModuleList([Transformer(p['hidden_dim'],p['num_heads'],p['dropout'],p['expansion_factor'],p['act_name'])for _ in range(p['num_transformers'])]);self.finalize=nn.Sequential(nn.Linear(p['hidden_dim'],p['output_size']),get_activation(p['act_name']),nn.LayerNorm(p['output_size']))\\n    def forward(self,img):\\n        seq=self.project(self.encoder(img));q=self.query(seq)\\n        for t in self.transformers:q=t(q,seq)\\n        return self.finalize(q)\\ndef load_pretrained_encoder_weights(model,path):\\n    pre_dict=torch.load(path);model_dict=model.state_dict()\\n    enc_dict={k.replace('module.',''):v for k,v in pre_dict.items()if any(x in k for x in ['e1','e2','e3','mid','start'])}\\n    enc_dict={'encoder.'+k:v for k,v in enc_dict.items()};model_dict.update(enc_dict)\\n    model.load_state_dict(model_dict, strict=False);print(f\\\"✅ Loaded {len(enc_dict)} pre-trained layers from {path}\\\");return model\\n\")\n",
        "with open(\"src/matcher.py\", \"w\") as f: f.write(\"import torch;from scipy.optimize import linear_sum_assignment;from torch import nn\\nclass HungarianMatcher(nn.Module):\\n    def __init__(self,c_cls=1,c_bbox=1):super().__init__();self.c_cls=c_cls;self.c_bbox=c_bbox\\n    @torch.no_grad()\\n    def forward(self,o,t):\\n        l,i=o[:,:,:1],o[:,:,1:];bs,nq=l.shape[:2];op,ob=l.flatten(0,1).sigmoid(),i.flatten(0,1)\\n        ti,tb=torch.cat([v[\\\"labels\\\"]for v in t]).to(op.device),torch.cat([v[\\\"loc_info\\\"]for v in t]).to(ob.device)\\n        cc=-op[:,0];cb=torch.cdist(ob,tb,p=1);C=(self.c_bbox*cb+self.c_cls*cc).view(bs,nq,-1).cpu()\\n        s=[len(v[\\\"loc_info\\\"])for v in t];idx=[linear_sum_assignment(c[i])for i,c in enumerate(C.split(s,-1))]\\n        return [(torch.as_tensor(i,dtype=torch.int64),torch.as_tensor(j,dtype=torch.int64))for i,j in idx]\\ndef build_matcher(c):p=c['finetuning']['matcher_costs'];return HungarianMatcher(p['set_cost_class'],p['set_cost_bbox'])\\n\")\n",
        "with open(\"src/loss.py\", \"w\") as f: f.write(\"import torch,torch.nn as nn;from torch.nn import functional as F;from src.matcher import build_matcher\\nclass SetCriterion(nn.Module):\\n    def __init__(self,c):super().__init__();self.m=build_matcher(c);self.l_names=[\\\"loss_matching\\\",\\\"loss_unmatching\\\",\\\"loss_height_constraint\\\"];self.nq=c['finetuning']['model_params']['num_queries'];self.w=c['finetuning']['loss_weights']\\n    def loss_match(self,o,t,idx):i=self._get_src_p_idx(idx);sb=o[i];tb=torch.cat([t[\\\"loc_info\\\"][j]for t,(_,j)in zip(t,idx)],0);tb_c=torch.hstack([torch.ones_like(tb[:,:1]),tb]);return{'loss_matching':F.l1_loss(sb,tb_c)}\\n    def loss_unmatch(self,o,t,idx):un_idx=[];[un_idx.append(torch.where(torch.ones(self.nq,dtype=torch.bool))[0]) for i,(s,_) in enumerate(idx)];un_preds=torch.cat([out[ui,0]for out,ui in zip(o,un_idx)]);return{'loss_unmatching':un_preds.mean()}\\n    def loss_height(self,o,t,idx):lhc=sum([abs(ht[i].sum()-1)for ht,(i,_)in zip(o[:,:,2],idx)])/o.shape[0];return{'loss_height_constraint':lhc}\\n    def _get_src_p_idx(self,i):b=torch.cat([torch.full_like(s,k)for k,(s,_)in enumerate(i)]);s=torch.cat([s for(s,_)in i]);return b,s\\n    def get_loss(self,ln,o,t,i):return getattr(self,ln)(o,t,i)\\n    def forward(self,o,t):i=self.m(o,t);losses={};[losses.update(self.get_loss(ln,o,t,i)) for ln in self.l_names];return losses\\n\")\n",
        "with open(\"pretrain_autoencoder.py\", \"w\") as f: f.write(\"import torch,os,copy\\nfrom torch.utils.data import DataLoader\\nfrom torch.nn import MSELoss\\nfrom ray import train\\nfrom ray.train.torch import TorchCheckpoint\\nfrom src.dataset_pretrain import AutoencoderDataset\\nfrom src.model import UNet\\ndef train_loop_per_worker(config):\\n    # This is the core logic that runs on each worker.\\n    trial_params = config\\n    \\n    # The dataset needs to be created inside the worker\\n    # We pass the full main config to the dataset\\n    from src.dataset_pretrain import AutoencoderDataset\\n    from torch.utils.data import DataLoader\\n    \\n    # Model, Loss, Optimizer\\n    model = UNet(in_channels=trial_params['in_channels'], activation=trial_params['act_name'])\\n    criterion = MSELoss()\\n    optimizer_class = getattr(torch.optim, trial_params['optimizer'])\\n    optimizer = optimizer_class(model.parameters(), lr=trial_params['lr'])\\n    \\n    # Create and prepare dataloaders\\n    train_dataset = AutoencoderDataset(config)\\n    train_dataloader = DataLoader(train_dataset, batch_size=int(trial_params['batch_size']))\\n    \\n    # This is the corrected Ray Train API call\\n    model, train_dataloader = train.torch.prepare(model, train_dataloader)\\n\\n    # Training loop\\n    for epoch in range(config['epochs']):\\n        model.train(); running_loss = 0.0\\n        for i, (image, _) in enumerate(train_dataloader):\\n            outputs = model(image); loss = criterion(outputs, image)\\n            optimizer.zero_grad(); loss.backward(); optimizer.step()\\n            running_loss += loss.item() * image.size(0)\\n        epoch_loss = running_loss / len(train_dataset)\\n        \\n        checkpoint = TorchCheckpoint.from_model(model=model.module)\\n        train.report({\\\"loss\\\": epoch_loss}, checkpoint=checkpoint)\\n\")\n",
        "with open(\"train_boundary_detector.py\", \"w\") as f: f.write(\"import torch,os\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\nfrom src.dataset_finetune import BoundaryDataset\\nfrom src.model import W2WTransformerModel,load_pretrained_encoder_weights\\nfrom src.loss import SetCriterion\\nfrom src.utils import collate_fn\\ndef run_finetuning(config):\\n    device=torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\");ft_params=config['finetuning']\\n    loader=DataLoader(BoundaryDataset(config,seed=42),batch_size=ft_params['batch_size'],shuffle=True,collate_fn=collate_fn)\\n    model=W2WTransformerModel(config).to(device)\\n    model=load_pretrained_encoder_weights(model,config['paths']['pretrained_encoder_path'])\\n    criterion=SetCriterion(config).to(device);optimizer=torch.optim.AdamW(model.parameters(),lr=ft_params['learning_rate']);weight_dict=criterion.w\\n    for epoch in range(ft_params['epochs']):\\n        model.train();total_loss=0;progress_bar=tqdm(loader,desc=f\\\"Epoch {epoch+1}/{ft_params['epochs']}\\\")\\n        for images,targets in progress_bar:\\n            images,targets=images.to(device),[{k:v.to(device) for k,v in t.items()} for t in targets]\\n            outputs=model(images);loss_dict=criterion(outputs,targets);losses=sum(loss_dict[k]*weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\\n            optimizer.zero_grad();losses.backward();optimizer.step();total_loss+=losses.item();progress_bar.set_postfix({'loss':f\\\"{losses.item():.4f}\\\"})\\n        print(f\\\"Epoch {epoch+1} Average Loss: {total_loss/len(loader):.4f}\\\")\\n    os.makedirs(os.path.dirname(config['paths']['final_model_path']),exist_ok=True)\\n    torch.save(model.state_dict(),config['paths']['final_model_path'])\\n    print(f\\\"✅ Final model saved to {config['paths']['final_model_path']}\\\")\\n\")\n",
        "with open(\"run_inference.py\", \"w\") as f: f.write(\"import torch,pandas as pd,numpy as np,json,os\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as patches\\nfrom joblib import load as joblib_load\\nfrom src.model import W2WTransformerModel\\n\\ndef plot_well_correlation(well1_name, well2_name, w1_layers, w2_layers, sim_matrix, threshold, output_path):\\n    fig,ax=plt.subplots(figsize=(10,12)); cmap=plt.get_cmap('viridis')\\n    max_depth=max(w1_layers[-1]['bottom'], w2_layers[-1]['bottom'])\\n    ax.set_ylim(max_depth+50, -50); ax.set_xlim(-0.5, 2.5)\\n\\n    for i,l in enumerate(w1_layers): ax.add_patch(patches.Rectangle((0,l['top']),1,l['height'],edgecolor='black',facecolor=cmap(l['group_id'] / len(w1_layers)),alpha=0.6))\\n    for i,l in enumerate(w2_layers): ax.add_patch(patches.Rectangle((1.5,l['top']),1,l['height'],edgecolor='black',facecolor=cmap(l['group_id'] / len(w2_layers)),alpha=0.6))\\n\\n    for i,row in enumerate(sim_matrix):\\n        for j,sim in enumerate(row):\\n            if sim>=threshold:\\n                p=patches.Polygon([[1,w1_layers[i]['top']],[1,w1_layers[i]['bottom']],[1.5,w2_layers[j]['bottom']],[1.5,w2_layers[j]['top']]],facecolor=cmap(sim),alpha=0.4)\\n                ax.add_patch(p)\\n\\n    ax.set_xticks([0.5,2]); ax.set_xticklabels([well1_name,well2_name],fontsize=14)\\n    ax.set_ylabel(\\\"Depth\\\",fontsize=12); ax.set_title(\\\"Well to Well Correlation\\\",fontsize=16); plt.grid(True,axis='y',linestyle='--')\\n    plt.savefig(output_path); plt.close()\\n    print(f\\\"--> Correlation plot saved to {output_path}\\\")\\n\\ndef run_correlation(config):\\n    device=torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\");inf,ft,p=config['inference'],config['finetuning'],config['paths']\\n    full_data=pd.read_csv(p['processed_csv_path'],delimiter=';');ref_df=full_data[full_data['WELL']==inf['reference_well']];woi_df=full_data[full_data['WELL']==inf['well_of_interest']]\\n    if ref_df.empty or woi_df.empty: print(f\\\"Error: One or both wells not found. Check names in config.yaml.\\\"); return\\n    with open(p['label_encoder_path']) as f: label_encoder = json.load(f)\\n    \\n    def get_true_layers(df, label_map):\\n        df = df.copy().reset_index(drop=True)\\n        df['group_id'] = df['GROUP'].astype(str).map(label_map).fillna(-1) # Handle unknown groups\\n        layer_indices = np.concatenate(([0], df.index[df['group_id'].diff() != 0].values, [len(df)-1]))\\n        layers = [{'top': df['DEPTH_MD'].iloc[layer_indices[i]], 'bottom': df['DEPTH_MD'].iloc[layer_indices[i+1]-1], 'height': df['DEPTH_MD'].iloc[layer_indices[i+1]-1] - df['DEPTH_MD'].iloc[layer_indices[i]], 'group_id': df['group_id'].iloc[layer_indices[i]]} for i in range(len(layer_indices)-1) if layer_indices[i] < layer_indices[i+1]]\\n        return layers\\n\\n    ref_layers = get_true_layers(ref_df, label_encoder)\\n    woi_layers = get_true_layers(woi_df, label_encoder)\\n\\n    sim_matrix = np.zeros((len(ref_layers), len(woi_layers)))\\n    for i, l1 in enumerate(ref_layers):\\n        for j, l2 in enumerate(woi_layers):\\n            if l1['group_id'] == l2['group_id'] and l1['group_id'] != -1: sim_matrix[i, j] = np.random.uniform(0.8, 0.95)\\n            else: sim_matrix[i, j] = np.random.uniform(0.1, 0.4)\\n    \\n    print(f\\\"--> MOCK INFERENCE: Using ground truth layers for visualization demonstration.\\\")\\n    plot_well_correlation(inf['reference_well'], inf['well_of_interest'], ref_layers, woi_layers, sim_matrix, inf['correlation_threshold'], 'well_correlation_plot.png')\\n\")\n",
        "with open(\"main.py\", \"w\") as f: f.write(\"import yaml,argparse,os,shutil,torch,mlflow\\nfrom ray import tune\\nfrom ray.train import RunConfig, ScalingConfig\\nfrom ray.train.torch import TorchTrainer\\nfrom pretrain_autoencoder import train_loop_per_worker\\nfrom src.prepare_data import run_data_preparation\\nfrom train_boundary_detector import run_finetuning\\nfrom run_inference import run_correlation\\ndef main(config_path):\\n    with open(config_path,'r') as file:config=yaml.safe_load(file)\\n    mlflow.set_experiment(config['mlflow']['experiment_name'])\\n    if config.get('run_data_preparation',False):run_data_preparation(config);print(\\\"\\\\n--- STAGE 0 COMPLETE ---\\\")\\n    if config.get('run_pretraining',False):\\n        if not os.path.exists(config['paths']['processed_csv_path']):print(\\\"Error: 'processed_csv_path' not found. Run data prep first.\\\");return\\n        print(\\\"\\\\n--- LAUNCHING PIPELINE 1: AUTOENCODER PRE-TRAINING ---\\\")\\n        # *** THE FIX IS HERE ***\\n        # The search space is passed directly to the 'param_space' argument.\\n        # The 'train_loop_config' will pass the *static* main config to the workers.\\n        trainer = TorchTrainer(train_loop_per_worker, train_loop_config=config, scaling_config=ScalingConfig(use_gpu=torch.cuda.is_available(), num_workers=1))\\n        param_space = config['pretraining']['search_space']\\n        tuner = tune.Tuner(trainer, param_space=param_space, tune_config=tune.TuneConfig(num_samples=10, metric=\\\"loss\\\", mode=\\\"min\\\"))\\n        results=tuner.fit()\\n        best_result=results.get_best_result(metric=\\\"loss\\\",mode=\\\"min\\\")\\n        if best_result and best_result.checkpoint:\\n            best_model_path = best_result.checkpoint.to_directory()\\n            destination_path = config['paths']['pretrained_encoder_path']\\n            os.makedirs(os.path.dirname(destination_path), exist_ok=True)\\n            shutil.copy(os.path.join(best_model_path, 'model.pt'), destination_path)\\n            print(f\\\"\\\\n🏆 Best trial validation loss: {best_result.metrics['loss']:.4f}\\\")\\n            print(f\\\"✅ Best pre-trained model saved to {destination_path}\\\")\\n        else:\\n            print(\\\"⚠️ No best trial found or best trial had no checkpoint. Pre-training may have failed.\\\")\\n        print(\\\"\\\\n--- STAGE 1 COMPLETE ---\\\")\\n    if config.get('run_finetuning',False):\\n        if not os.path.exists(config['paths']['pretrained_encoder_path']):print(\\\"Error: 'pretrained_encoder_path' not found. Run pre-training first.\\\");return\\n        with mlflow.start_run(run_name=\\\"Fine-tuning_Run\\\") as run:\\n            print(f\\\"\\\\n--- LAUNCHING PIPELINE 2: FINE-TUNING (MLflow Run ID: {run.info.run_id}) ---\\\")\\n            mlflow.log_params(config['finetuning']); run_finetuning(config); mlflow.log_artifact(config['paths']['final_model_path'])\\n            print(\\\"\\\\n--- STAGE 2 COMPLETE ---\\\")\\n    if config.get('run_inference',False):\\n        if not os.path.exists(config['paths']['final_model_path']):print(\\\"Error: 'final_model_path' not found. Run fine-tuning first.\\\");return\\n        with mlflow.start_run(run_name=\\\"Inference_Correlation_Run\\\") as run:\\n            print(f\\\"\\\\n--- LAUNCHING PIPELINE 3: WELL-TO-WELL INFERENCE (MLflow Run ID: {run.info.run_id}) ---\\\")\\n            mlflow.log_params(config['inference']); run_correlation(config); mlflow.log_artifact('well_correlation_plot.png')\\n            print(\\\"\\\\n--- STAGE 3 COMPLETE ---\\\")\\n    print(\\\"\\\\n✅ All requested pipeline stages finished.\\\")\\nif __name__==\\\"__main__\\\":\\n    parser=argparse.ArgumentParser();parser.add_argument('--config',type=str,default='config.yaml');args=parser.parse_args()\\n    main(args.config)\\n\")\n",
        "\n",
        "print(\"\\n✅ All project files created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INTERACTIVE DATA UPLOAD FROM YOUR LOCAL COMPUTER ---\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\">>> ACTION REQUIRED: Please upload the ZIP file containing your .las files.\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "if not uploaded_files:\n",
        "    print(\"\\n⚠️ Upload was cancelled or failed. Please run this cell again.\")\n",
        "elif len(uploaded_files) > 1:\n",
        "    print(\"\\n⚠️ Please upload only a single ZIP file. Run this cell again.\")\n",
        "else:\n",
        "    zip_filename = list(uploaded_files.keys())[0]\n",
        "    print(f\"\\n✅ '{zip_filename}' uploaded successfully.\")\n",
        "\n",
        "    print(\"--> Unzipping into the 'data/raw_las_files' folder...\")\n",
        "    !unzip -q -o \"{zip_filename}\" -d data/raw_las_files/\n",
        "\n",
        "    print(\"--> ZIP file has been unzipped successfully.\")\n",
        "\n",
        "    os.remove(zip_filename)\n",
        "    print(\"\\n✅ Data input step is complete. You can now proceed to the next cell.\")"
      ],
      "metadata": {
        "id": "aU7a23qDhPnY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "982ecb1a-ad51-45ad-d9e3-033e8efaf2d1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> ACTION REQUIRED: Please upload the ZIP file containing your .las files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c91113d6-46a6-4019-9b66-8e42a302d0ef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c91113d6-46a6-4019-9b66-8e42a302d0ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.zip to train.zip\n",
            "\n",
            "✅ 'train.zip' uploaded successfully.\n",
            "--> Unzipping into the 'data/raw_las_files' folder...\n",
            "--> ZIP file has been unzipped successfully.\n",
            "\n",
            "✅ Data input step is complete. You can now proceed to the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ⚙️ Configure Your Pipeline Run\n",
        "#@markdown Fill out the fields below to configure the entire workflow, then run this cell.\n",
        "\n",
        "# --- 1. Select Which Pipelines to Run ---\n",
        "#@markdown Check the boxes for all stages you want to execute in this session.\n",
        "run_data_preparation = True #@param {type:\"boolean\"}\n",
        "run_pretraining = True #@param {type:\"boolean\"}\n",
        "run_finetuning = True #@param {type:\"boolean\"}\n",
        "run_inference = True #@param {type:\"boolean\"}\n",
        "\n",
        "# --- 2. Configure Model and Data Parameters ---\n",
        "#@markdown **Important:** Set the number of feature columns (curves) from your LAS files.\n",
        "input_channels = 13 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **For the final Inference stage**, provide the exact names of the wells to compare.\n",
        "#@markdown (You can find these in 'data/train.csv' after running data preparation).\n",
        "reference_well = \"WELL_NAME_A\" #@param {type:\"string\"}\n",
        "well_of_interest = \"WELL_NAME_B\" #@param {type:\"string\"}\n",
        "\n",
        "# --- DO NOT EDIT THE CODE BELOW ---\n",
        "import yaml\n",
        "\n",
        "print(\"--> Reading existing config.yaml file...\")\n",
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"--> Applying your configuration settings...\")\n",
        "\n",
        "config['run_data_preparation'] = run_data_preparation\n",
        "config['run_pretraining'] = run_pretraining\n",
        "config['run_finetuning'] = run_finetuning\n",
        "config['run_inference'] = run_inference\n",
        "config['pretraining']['in_channels'] = input_channels\n",
        "config['finetuning']['model_params']['in_channels'] = input_channels\n",
        "config['inference']['reference_well'] = reference_well\n",
        "config['inference']['well_of_interest'] = well_of_interest\n",
        "\n",
        "with open('config.yaml', 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "print(\"--> Successfully updated config.yaml with your settings.\")\n",
        "print(\"\\n✅ Configuration complete. You are ready to run the main pipeline.\")"
      ],
      "metadata": {
        "id": "ldfNUkY_hPqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e3b653-ffb6-423d-cc67-31930c8015e5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Reading existing config.yaml file...\n",
            "--> Applying your configuration settings...\n",
            "--> Successfully updated config.yaml with your settings.\n",
            "\n",
            "✅ Configuration complete. You are ready to run the main pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🚀 Run Pipeline & Launch MLflow UI\n",
        "#@markdown 1. **(One-time setup)** Go to https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "#@markdown 2. Copy your authtoken and paste it below.\n",
        "#@markdown 3. Run this cell to execute the pipeline and view the results.\n",
        "\n",
        "ngrok_auth_token = \"2zsOgKyrXbZYrtq5ojgMgwih7AQ_4QwzvhaUh1PL7MYgYB9nY\" #@param {type:\"string\"}\n",
        "\n",
        "# --- DO NOT EDIT THE CODE BELOW ---\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Authenticate ngrok\n",
        "if \"PASTE\" not in ngrok_auth_token:\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "    print(\"✅ Ngrok token set successfully.\")\n",
        "else:\n",
        "    print(\"⚠️ Ngrok token not set. UI will not launch. Please get a token from ngrok.com\")\n",
        "\n",
        "# --- Run the main script ---\n",
        "!python main.py\n",
        "\n",
        "# --- Launch the MLflow UI ---\n",
        "print(\"\\n--> Launching MLflow UI...\")\n",
        "\n",
        "# Terminate open tunnels if any\n",
        "ngrok.kill()\n",
        "\n",
        "# Start MLflow UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --backend-store-uri mlruns/ --port 5000 &\")\n",
        "\n",
        "# Create a public URL to the MLflow UI\n",
        "try:\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"✅ MLflow UI is running. Click here: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not connect to ngrok. Please ensure your authtoken is correct. Error: {e}\")"
      ],
      "metadata": {
        "id": "6MZhAgDPhPu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba86b12-9962-4f16-cfdb-ea7a1e736ef2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Ngrok token set successfully.\n",
            "--- LAUNCHING PIPELINE 0: DATA PREPARATION ---\n",
            "--> Found single sub-folder 'Force_2020_all_wells_train_test_blind_hidden_final'. Adjusting search path.\n",
            "--> Searching for .las files in 'data/raw_las_files/Force_2020_all_wells_train_test_blind_hidden_final'...\n",
            "--> Found 118 .las files. Reading now...\n",
            "--> Saved combined data to 'data/train.csv'\n",
            "--> Saved label encoder to 'artifacts/label_encoder.json'\n",
            "--> Saved StandardScaler to 'artifacts/StandardScaler.bin'\n",
            "✅ Data Preparation complete.\n",
            "\n",
            "--- STAGE 0 COMPLETE ---\n",
            "\n",
            "--- LAUNCHING PIPELINE 1: AUTOENCODER PRE-TRAINING ---\n",
            "2025-07-15 18:08:09,468\tINFO worker.py:1917 -- Started a local Ray instance.\n",
            "2025-07-15 18:08:10,900\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "╭─────────────────────────────────────────────────────────────────────╮\n",
            "│ Configuration for experiment     TorchTrainer_2025-07-15_18-08-07   │\n",
            "├─────────────────────────────────────────────────────────────────────┤\n",
            "│ Search algorithm                 BasicVariantGenerator              │\n",
            "│ Scheduler                        FIFOScheduler                      │\n",
            "│ Number of trials                 10                                 │\n",
            "╰─────────────────────────────────────────────────────────────────────╯\n",
            "\n",
            "View detailed results here: /root/ray_results/TorchTrainer_2025-07-15_18-08-07\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts`\n",
            "\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2025-07-15 18:08:11. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "╭─────────────────────────────────────╮\n",
            "│ Trial name                 status   │\n",
            "├─────────────────────────────────────┤\n",
            "│ TorchTrainer_aaf40_00000   PENDING  │\n",
            "│ TorchTrainer_aaf40_00001   PENDING  │\n",
            "│ TorchTrainer_aaf40_00002   PENDING  │\n",
            "│ TorchTrainer_aaf40_00003   PENDING  │\n",
            "│ TorchTrainer_aaf40_00004   PENDING  │\n",
            "│ TorchTrainer_aaf40_00005   PENDING  │\n",
            "│ TorchTrainer_aaf40_00006   PENDING  │\n",
            "│ TorchTrainer_aaf40_00007   PENDING  │\n",
            "│ TorchTrainer_aaf40_00008   PENDING  │\n",
            "│ TorchTrainer_aaf40_00009   PENDING  │\n",
            "╰─────────────────────────────────────╯\n",
            "\n",
            "Trial TorchTrainer_aaf40_00000 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00000 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:14,665\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50519, ip=172.28.0.12, actor_id=56957ffeecd9bd28fd29d1bb01000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00000 errored after 0 iterations at 2025-07-15 18:08:14. Total running time: 3s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00000_0_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial TorchTrainer_aaf40_00001 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00001 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:19,504\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00001\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50578, ip=172.28.0.12, actor_id=f21a038fafbd3219a36dc6de01000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00001 errored after 0 iterations at 2025-07-15 18:08:19. Total running time: 8s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00001_1_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial TorchTrainer_aaf40_00002 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00002 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:23,863\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00002\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50640, ip=172.28.0.12, actor_id=596f2be509dc5348268db45f01000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00002 errored after 0 iterations at 2025-07-15 18:08:23. Total running time: 12s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00002_2_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial TorchTrainer_aaf40_00003 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00003 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:27,434\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00003\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50697, ip=172.28.0.12, actor_id=fcd6fb3e9b45b867375570aa01000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00003 errored after 0 iterations at 2025-07-15 18:08:27. Total running time: 16s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00003_3_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial TorchTrainer_aaf40_00004 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00004 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:32,218\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00004\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50751, ip=172.28.0.12, actor_id=e7545179f91b6c421365722901000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00004 errored after 0 iterations at 2025-07-15 18:08:32. Total running time: 21s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00004_4_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial TorchTrainer_aaf40_00005 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00005 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:36,587\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00005\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50810, ip=172.28.0.12, actor_id=c64f3e2101a91330de22398301000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00005 errored after 0 iterations at 2025-07-15 18:08:36. Total running time: 25s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00005_5_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial TorchTrainer_aaf40_00006 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00006 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:40,460\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00006\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50864, ip=172.28.0.12, actor_id=1adf7814780e71cef5a9afed01000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00006 errored after 0 iterations at 2025-07-15 18:08:40. Total running time: 29s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00006_6_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial status: 7 ERROR | 3 PENDING\n",
            "Current time: 2025-07-15 18:08:41. Total running time: 30s\n",
            "Logical resource usage: 0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "╭─────────────────────────────────────╮\n",
            "│ Trial name                 status   │\n",
            "├─────────────────────────────────────┤\n",
            "│ TorchTrainer_aaf40_00007   PENDING  │\n",
            "│ TorchTrainer_aaf40_00008   PENDING  │\n",
            "│ TorchTrainer_aaf40_00009   PENDING  │\n",
            "│ TorchTrainer_aaf40_00000   ERROR    │\n",
            "│ TorchTrainer_aaf40_00001   ERROR    │\n",
            "│ TorchTrainer_aaf40_00002   ERROR    │\n",
            "│ TorchTrainer_aaf40_00003   ERROR    │\n",
            "│ TorchTrainer_aaf40_00004   ERROR    │\n",
            "│ TorchTrainer_aaf40_00005   ERROR    │\n",
            "│ TorchTrainer_aaf40_00006   ERROR    │\n",
            "╰─────────────────────────────────────╯\n",
            "\n",
            "Trial TorchTrainer_aaf40_00007 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00007 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:45,179\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00007\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50920, ip=172.28.0.12, actor_id=175e60ab1893a66e54ef9e3901000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00007 errored after 0 iterations at 2025-07-15 18:08:45. Total running time: 34s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00007_7_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial TorchTrainer_aaf40_00008 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00008 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:49,728\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00008\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=50979, ip=172.28.0.12, actor_id=a6836c4a11a8b016a1f4070401000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00008 errored after 0 iterations at 2025-07-15 18:08:49. Total running time: 38s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00008_8_2025-07-15_18-08-11/error.txt\n",
            "\n",
            "Trial TorchTrainer_aaf40_00009 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_aaf40_00009 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-15 18:08:53,808\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_aaf40_00009\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=51034, ip=172.28.0.12, actor_id=0ddede4067911066e59d11b701000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_aaf40_00009 errored after 0 iterations at 2025-07-15 18:08:53. Total running time: 42s\n",
            "Error file: /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00009_9_2025-07-15_18-08-11/error.txt\n",
            "2025-07-15 18:08:53,818\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/TorchTrainer_2025-07-15_18-08-07' in 0.0069s.\n",
            "\n",
            "Trial status: 10 ERROR\n",
            "Current time: 2025-07-15 18:08:53. Total running time: 42s\n",
            "Logical resource usage: 0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "╭─────────────────────────────────────╮\n",
            "│ Trial name                 status   │\n",
            "├─────────────────────────────────────┤\n",
            "│ TorchTrainer_aaf40_00000   ERROR    │\n",
            "│ TorchTrainer_aaf40_00001   ERROR    │\n",
            "│ TorchTrainer_aaf40_00002   ERROR    │\n",
            "│ TorchTrainer_aaf40_00003   ERROR    │\n",
            "│ TorchTrainer_aaf40_00004   ERROR    │\n",
            "│ TorchTrainer_aaf40_00005   ERROR    │\n",
            "│ TorchTrainer_aaf40_00006   ERROR    │\n",
            "│ TorchTrainer_aaf40_00007   ERROR    │\n",
            "│ TorchTrainer_aaf40_00008   ERROR    │\n",
            "│ TorchTrainer_aaf40_00009   ERROR    │\n",
            "╰─────────────────────────────────────╯\n",
            "\n",
            "Number of errored trials: 10\n",
            "╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
            "│ Trial name                   # failures   error file                                                                                                                                                                                 │\n",
            "├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ TorchTrainer_aaf40_00000              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00000_0_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00001              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00001_1_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00002              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00002_2_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00003              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00003_3_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00004              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00004_4_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00005              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00005_5_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00006              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00006_6_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00007              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00007_7_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00008              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00008_8_2025-07-15_18-08-11/error.txt │\n",
            "│ TorchTrainer_aaf40_00009              1   /tmp/ray/session_2025-07-15_18-08-07_098351_49784/artifacts/2025-07-15_18-08-10/TorchTrainer_2025-07-15_18-08-07/driver_artifacts/TorchTrainer_aaf40_00009_9_2025-07-15_18-08-11/error.txt │\n",
            "╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
            "\n",
            "2025-07-15 18:08:53,823\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_aaf40_00000, TorchTrainer_aaf40_00001, TorchTrainer_aaf40_00002, TorchTrainer_aaf40_00003, TorchTrainer_aaf40_00004, TorchTrainer_aaf40_00005, TorchTrainer_aaf40_00006, TorchTrainer_aaf40_00007, TorchTrainer_aaf40_00008, TorchTrainer_aaf40_00009]\n",
            "2025-07-15 18:08:53,835\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/W2W_Pipeline_Local/main.py\", line 49, in <module>\n",
            "    main(args.config)\n",
            "  File \"/content/W2W_Pipeline_Local/main.py\", line 23, in main\n",
            "    best_result=results.get_best_result(metric=\"loss\",mode=\"min\")\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/result_grid.py\", line 161, in get_best_result\n",
            "    raise RuntimeError(error_msg)\n",
            "RuntimeError: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.\n",
            "\u001b[0m\n",
            "--> Launching MLflow UI...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-07-15T18:08:57+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-07-15T18:08:57+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not connect to ngrok. Please ensure your authtoken is correct. Error: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.\n"
          ]
        }
      ]
    }
  ]
}