{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsY5T1cHamWvMTnbxzPumS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAHIL9581/w2w/blob/main/W2W_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RTdradEARztG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965c627c-b8e3-499c-e6e4-9ab96138d238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Creating a temporary project workspace at: /content/W2W_Pipeline_Local\n",
            "/content/W2W_Pipeline_Local\n",
            "--> Successfully changed directory to: /content/W2W_Pipeline_Local\n",
            "\n",
            "--> Installing all necessary Python libraries (this may take a few minutes)...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Library installation complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import textwrap\n",
        "\n",
        "# --- 1. SETUP THE TEMPORARY ENVIRONMENT AND WORKSPACE ---\n",
        "PROJECT_PATH = \"/content/W2W_Pipeline_Local\"\n",
        "print(f\"--> Creating a temporary project workspace at: {PROJECT_PATH}\")\n",
        "\n",
        "os.makedirs(f\"{PROJECT_PATH}/src\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/data/raw_las_files\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/artifacts\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/trained_models/autoencoder\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_PATH}/trained_models/boundary_detector\", exist_ok=True)\n",
        "\n",
        "# Change the current working directory to the project path\n",
        "%cd {PROJECT_PATH}\n",
        "print(f\"--> Successfully changed directory to: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# --- 2. INSTALL ALL REQUIRED LIBRARIES ---\n",
        "print(\"\\n--> Installing all necessary Python libraries (this may take a few minutes)...\")\n",
        "# Ensure latest versions and no conflicts\n",
        "!pip install -U \"ray[train,tune]>=2.9.0\" mlflow torch torchvision torchaudio lasio scikit-learn pandas tqdm matplotlib joblib pyyaml pyngrok -q\n",
        "print(\"✅ Library installation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import os\n",
        "\n",
        "print(\"--> Creating all project source files...\")\n",
        "\n",
        "# --- config.yaml ---\n",
        "with open(\"config.yaml\", \"w\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "    run_data_preparation: true\n",
        "    run_pretraining: true\n",
        "    run_finetuning: true\n",
        "    run_inference: true\n",
        "    paths:\n",
        "      raw_las_folder: \"data/raw_las_files/\"\n",
        "      processed_csv_path: \"data/train.csv\"\n",
        "      label_encoder_path: \"artifacts/label_encoder.json\"\n",
        "      std_scaler_path: \"artifacts/StandardScaler.bin\"\n",
        "      pretrained_encoder_path: \"trained_models/autoencoder/best_autoencoder.pt\"\n",
        "      final_model_path: \"trained_models/boundary_detector/final_model.pt\"\n",
        "    mlflow:\n",
        "      experiment_name: \"W2W_Matcher_Pipeline\"\n",
        "    pretraining:\n",
        "      epochs: 25\n",
        "      num_samples: 10\n",
        "      search_space:\n",
        "        in_channels: 13\n",
        "        optimizer: [\"RMSprop\", \"AdamW\", \"Adam\"]\n",
        "        lr: [0.001, 0.0001]\n",
        "        act_name: [\"prelu\", \"relu\"]\n",
        "        batch_size: [16, 32]\n",
        "    finetuning:\n",
        "      learning_rate: 0.0001\n",
        "      batch_size: 16\n",
        "      epochs: 100\n",
        "      model_params: {patch_height: 700, in_channels: 13, act_name: \"prelu\", project_in_features: 2048, hidden_dim: 256, num_queries: 100, num_heads: 8, dropout: 0.1, expansion_factor: 4, num_transformers: 6, output_size: 3}\n",
        "      matcher_costs: {set_cost_class: 1, set_cost_bbox: 5}\n",
        "      loss_weights: {loss_matching: 1.0, loss_unmatching: 0.5, loss_height_constraint: 0.5}\n",
        "    inference:\n",
        "      reference_well: \"WELL_NAME_A\"\n",
        "      well_of_interest: \"WELL_NAME_B\"\n",
        "      correlation_threshold: 0.7\n",
        "    \"\"\"))\n",
        "\n",
        "# --- ALL OTHER SCRIPTS ---\n",
        "# Create src directory if it doesn't exist\n",
        "os.makedirs(\"src\", exist_ok=True)\n",
        "\n",
        "with open(\"src/__init__.py\", \"w\") as f: f.write(\"# Makes this a package\\n\")\n",
        "with open(\"src/prepare_data.py\", \"w\") as f: f.write(\"import pandas as pd,numpy as np,lasio,os,json\\nfrom sklearn.preprocessing import StandardScaler\\nfrom joblib import dump\\ndef run_data_preparation(config):\\n    print(\\\"--- LAUNCHING PIPELINE 0: DATA PREPARATION ---\\\")\\n    paths = config['paths']\\n    search_folder = paths['raw_las_folder']\\n    items_in_folder = os.listdir(search_folder)\\n    # Check if there's a single sub-folder directly inside raw_las_files\\n    if len(items_in_folder) == 1 and os.path.isdir(os.path.join(search_folder, items_in_folder[0])):\\n        print(f\\\"--> Found single sub-folder '{items_in_folder[0]}'. Adjusting search path.\\\")\\n        search_folder = os.path.join(search_folder, items_in_folder[0])\\n    all_wells_df, las_files_found = [], []\\n    print(f\\\"--> Searching for .las files in '{search_folder}'...\\\")\\n    for root, dirs, files in os.walk(search_folder):\\n        for file in files:\\n            if file.lower().endswith('.las'): las_files_found.append(os.path.join(root, file))\\n    if not las_files_found: raise FileNotFoundError(f\\\"No .las files found in '{search_folder}'. Check your ZIP or folder structure.\\\")\\n    print(f\\\"--> Found {len(las_files_found)} .las files. Reading now...\\\")\\n    for filepath in las_files_found:\\n        try:\\n            las = lasio.read(filepath); df = las.df().reset_index()\\n            df['WELL'] = las.well.WELL.value if las.well.WELL.value else os.path.splitext(os.path.basename(filepath))[0]; df['GROUP'] = 'UNKNOWN'\\n            for param in las.params:\\n                if 'GROUP' in param.mnemonic: df['GROUP'] = param.value; break\\n            all_wells_df.append(df)\\n        except Exception as e: print(f\\\"    - Could not read {filepath}: {e}\\\")\\n    master_df = pd.concat(all_wells_df, ignore_index=True)\\n    if 'DEPT' in master_df.columns: master_df.rename(columns={'DEPT':'DEPTH_MD'}, inplace=True)\\n    master_df.to_csv(paths['processed_csv_path'], index=False, sep=';'); print(f\\\"--> Saved combined data to '{paths['processed_csv_path']}'\\\")\\n    label_encoder={str(g):i for i, g in enumerate(master_df['GROUP'].unique())}\\n    with open(paths['label_encoder_path'], 'w') as f: json.dump(label_encoder, f, indent=4)\\n    print(f\\\"--> Saved label encoder to '{paths['label_encoder_path']}'\\\")\\n    numeric_df = master_df.drop(columns=['WELL', 'GROUP', 'DEPTH_MD'], errors='ignore')\\n    scaler = StandardScaler(); scaler.fit(numeric_df.fillna(0)); dump(scaler, paths['std_scaler_path'])\\n    print(f\\\"--> Saved StandardScaler to '{paths['std_scaler_path']}'\\\"); print(\\\"✅ Data Preparation complete.\\\")\\n\")\n",
        "with open(\"src/utils.py\", \"w\") as f: f.write(\"import torch\\ndef collate_fn(batch):images,t=zip(*batch);return torch.stack(images),list(t)\\n\")\n",
        "with open(\"src/dataset_pretrain.py\", \"w\") as f: f.write(\"import numpy as np,torch,pandas as pd\\nfrom torch.utils import data\\nfrom joblib import load\\nclass AutoencoderDataset(data.Dataset):\\n    def __init__(self,c):\\n        p=c['paths'];df=pd.read_csv(p['processed_csv_path'],delimiter=';')\\n        cols_drop=['WELL','GROUP','DEPTH_MD']\\n        df.drop(columns=cols_drop,inplace=True,errors='ignore');df.fillna(0,inplace=True)\\n        scaler=load(p['std_scaler_path']);self.data=scaler.transform(df).astype(np.float32)\\n    def __len__(self):return len(self.data)\\n    def __getitem__(self,i):s=self.data[i];return torch.from_numpy(s),torch.from_numpy(s)\\n\")\n",
        "with open(\"src/dataset_finetune.py\", \"w\") as f: f.write(\"import json,numpy as np,torch,pandas as pd\\nfrom torch.utils import data\\nfrom joblib import load\\nclass BoundaryDataset(data.Dataset):\\n    def __init__(self,c,seed=None):\\n        self.p=c['finetuning']['model_params'];self.d=c['paths'];self.s=seed if seed else np.random.randint(2**32-1)\\n        x,y=self.get_Xy();self.x=x;self.gt=y\\n    def load_df(self,p,d=';'):return pd.read_csv(p,delimiter=d)\\n    def get_rand_well(self,d,s):np.random.seed(s);names=list(d.WELL.unique());idx=np.random.randint(0,len(names));return d[d['WELL']==names[idx]].copy()\\n    def get_gt_b(self,y_l):\\n        gts=[];\\n        for n,y in enumerate(y_l):\\n            gt,c={},0;k=[i+1 for i in range(len(y)-1) if not y[i]==y[i+1]];k.insert(0,0);gp=[y[idx] for idx in k];top=k.copy();k.append(len(y));h=[e1-e2 for(e1,e2) in zip(k[1:],k[:-1])]\\n            for t,h_val,g in zip(top,h,gp):gt[c]={'Group':int(g),'Top':int(t),'Height':int(h_val)};c+=1\\n            gts.append(gt)\\n        return gts\\n    def get_Xy(self):\\n        d=self.load_df(self.d['processed_csv_path']);w=self.get_rand_well(d,self.s)\\n        with open(self.d['label_encoder_path']) as f:le=json.load(f)\\n        w.loc[:,'GROUP']=w['GROUP'].astype(str).map(le).bfill().ffill()\\n        lbl=w['GROUP'].copy()\\n        cols_drop=['WELL','GROUP','DEPTH_MD']\\n        w_numeric=w.drop(columns=cols_drop,errors='ignore');w_numeric.fillna(0,inplace=True)\\n        scaler=load(self.d['std_scaler_path']);s_d=scaler.transform(w_numeric)\\n        ph=self.p['patch_height'];idx=list(range(0,s_d.shape[0],ph))\\n        x=np.asarray([s_d[i:i+ph] for i in idx if s_d[i:i+ph].shape[0]==ph]).astype(np.float32)\\n        y=np.asarray([lbl.values[i:i+ph] for i in idx if lbl.values[i:i+ph].shape[0]==ph])\\n        return x,self.get_gt_b(y)\\n    def __len__(self):return len(self.x)\\n    def __getitem__(self,idx):\\n        img=np.expand_dims(self.x[idx],0);data=self.gt[idx];lbl,top,h=[],[],[]\\n        for i in data:top.append(data[i]['Top']/self.p['patch_height']);h.append(data[i]['Height']/self.p['patch_height']);lbl.append(1)\\n        tgt={};tgt['labels']=torch.tensor(lbl,dtype=torch.long);t,h_v=torch.tensor(top,dtype=torch.float32).view(-1,1),torch.tensor(h,dtype=torch.float32).view(-1,1)\\n        tgt['loc_info']=torch.hstack((t,h_v));return torch.from_numpy(img),tgt\\n\")\n",
        "with open(\"src/model.py\", \"w\") as f: f.write(\"import torch,torch.nn as nn\\ndef get_activation(name): return nn.PReLU() if name=='prelu' else nn.ReLU() if name=='relu' else nn.GELU()\\nclass Project(nn.Module):\\n    def __init__(self,i,o): super().__init__(); self.l=nn.Linear(i,o)\\n    def forward(self,x): return self.l(x.flatten(1))\\nclass Query(nn.Module):\\n    def __init__(self,s,d): super().__init__(); self.q=nn.Parameter(torch.randn(1,s,d))\\n    def forward(self,x): return self.q.repeat(x.shape[0],1,1)\\nclass Transformer(nn.Module):\\n    def __init__(self,i,n,d,e,a): super().__init__(); self.t=nn.TransformerEncoderLayer(d_model=i,nhead=n,dropout=d,batch_first=True)\\n    def forward(self,q,c): return self.t(q)\\nclass Block(nn.Module):\\n    def __init__(self,i,o,s=2,k=3,a='prelu'):\\n        super().__init__(); p=k//2; self.act=get_activation(a); self.b=nn.Sequential(nn.Conv2d(i,o,k,s,p),nn.BatchNorm2d(o),self.act,nn.Conv2d(o,o,k,1,p),nn.BatchNorm2d(o),self.act)\\n    def forward(self,x): return self.b(x)\\nclass UNet(nn.Module):\\n    def __init__(self,in_channels=13,activation='prelu'):\\n        super().__init__();self.act=get_activation(activation);self.start=nn.Sequential(nn.Conv2d(in_channels,32,3,1,1),nn.BatchNorm2d(32),self.act);self.e1=Block(32,64,2,a=activation);self.e2=Block(64,128,2,a=activation);self.e3=Block(128,256,2,a=activation);self.mid=nn.Sequential(nn.Conv2d(256,512,2),nn.BatchNorm2d(512),self.act);self.uc3=nn.ConvTranspose2d(512,256,2,2);self.d3=Block(512,256,1,a=activation);self.uc2=nn.ConvTranspose2d(256,128,2,2);self.d2=Block(256,128,1,a=activation);self.uc1=nn.ConvTranspose2d(128,64,2,2);self.d1=Block(128,64,1,a=activation);self.out=nn.Conv2d(64,in_channels,1)\\n    def forward(self,x):\\n        x=x.unsqueeze(-1).unsqueeze(-1);x1=self.e1(self.start(x));x2=self.e2(x1);x3=self.e3(x2);m=self.mid(x3);u3=self.d3(torch.cat((self.uc3(m,output_size=x3.size()),x3),1));u2=self.d2(torch.cat((self.uc2(u3,output_size=x2.size()),x2),1));u1=self.d1(torch.cat((self.uc1(u2,output_size=x1.size()),x1),1));return self.out(u1).squeeze(-1).squeeze(-1)\\nclass UNetEncoder(nn.Module):\\n    def __init__(self,in_channels=13,activation='prelu'):\\n        super().__init__();self.act=get_activation(activation);self.start=nn.Sequential(nn.Conv2d(in_channels,32,3,1,1),nn.BatchNorm2d(32),self.act);self.e1=Block(32,64,2,a=activation);self.e2=Block(64,128,2,a=activation);self.e3=Block(128,256,2,a=activation);self.mid=nn.Sequential(nn.Conv2d(256,512,2),nn.BatchNorm2d(512),self.act)\\n    def forward(self,x):x=x.unsqueeze(-1);x=x.permute(0,2,1);x=x.unsqueeze(-1);x1=self.e1(self.start(x));x2=self.e2(x1);x3=self.e3(x2);m=self.mid(x3);return m\\nclass W2WTransformerModel(nn.Module):\\n    def __init__(self,c):\\n        super().__init__();p=c['finetuning']['model_params'];self.encoder=UNetEncoder(p['in_channels'],p['act_name']);self.project=Project(p['project_in_features'],p['hidden_dim']);self.query=Query(p['num_queries'],p['hidden_dim']);self.transformers=nn.ModuleList([Transformer(p['hidden_dim'],p['num_heads'],p['dropout'],p['expansion_factor'],p['act_name'])for _ in range(p['num_transformers'])]);self.finalize=nn.Sequential(nn.Linear(p['hidden_dim'],p['output_size']),get_activation(p['act_name']),nn.LayerNorm(p['output_size']))\\n    def forward(self,img):\\n        seq=self.project(self.encoder(img));q=self.query(seq)\\n        for t in self.transformers:q=t(q,seq)\\n        return self.finalize(q)\\ndef load_pretrained_encoder_weights(model,path):\\n    pre_dict=torch.load(path);model_dict=model.state_dict()\\n    enc_dict={k.replace('module.',''):v for k,v in pre_dict.items()if any(x in k for x in ['e1','e2','e3','mid','start'])}\\n    enc_dict={'encoder.'+k:v for k,v in enc_dict.items()};model_dict.update(enc_dict)\\n    model.load_state_dict(model_dict, strict=False);print(f\\\"✅ Loaded {len(enc_dict)} pre-trained layers from {path}\\\");return model\\n\")\n",
        "with open(\"src/matcher.py\", \"w\") as f: f.write(\"import torch;from scipy.optimize import linear_sum_assignment;from torch import nn\\nclass HungarianMatcher(nn.Module):\\n    def __init__(self,c_cls=1,c_bbox=1):super().__init__();self.c_cls=c_cls;self.c_bbox=c_bbox\\n    @torch.no_grad()\\n    def forward(self,o,t):\\n        l,i=o[:,:,:1],o[:,:,1:];bs,nq=l.shape[:2];op,ob=l.flatten(0,1).sigmoid(),i.flatten(0,1)\\n        ti,tb=torch.cat([v[\\\"labels\\\"]for v in t]).to(op.device),torch.cat([v[\\\"loc_info\\\"]for v in t]).to(ob.device)\\n        cc=-op[:,0];cb=torch.cdist(ob,tb,p=1);C=(self.c_bbox*cb+self.c_cls*cc).view(bs,nq,-1).cpu()\\n        s=[len(v[\\\"loc_info\\\"])for v in t];idx=[linear_sum_assignment(c[i])for i,c in enumerate(C.split(s,-1))]\\n        return [(torch.as_tensor(i,dtype=torch.int64),torch.as_tensor(j,dtype=torch.int64))for i,j in idx]\\ndef build_matcher(c):p=c['finetuning']['matcher_costs'];return HungarianMatcher(p['set_cost_class'],p['set_cost_bbox'])\\n\")\n",
        "with open(\"src/loss.py\", \"w\") as f: f.write(\"import torch,torch.nn as nn;from torch.nn import functional as F;from src.matcher import build_matcher\\nclass SetCriterion(nn.Module):\\n    def __init__(self,c):super().__init__();self.m=build_matcher(c);self.l_names=[\\\"loss_matching\\\",\\\"loss_unmatching\\\",\\\"loss_height_constraint\\\"];self.nq=c['finetuning']['model_params']['num_queries'];self.w=c['finetuning']['loss_weights']\\n    def loss_match(self,o,t,idx):i=self._get_src_p_idx(idx);sb=o[i];tb=torch.cat([t[\\\"loc_info\\\"][j]for t,(_,j)in zip(t,idx)],0);tb_c=torch.hstack([torch.ones_like(tb[:,:1]),tb]);return{'loss_matching':F.l1_loss(sb,tb_c)}\\n    def loss_unmatch(self,o,t,idx):un_idx=[];[un_idx.append(torch.where(torch.ones(self.nq,dtype=torch.bool))[0]) for i,(s,_) in enumerate(idx)];un_preds=torch.cat([out[ui,0]for out,ui in zip(o,un_idx)]);return{'loss_unmatching':un_preds.mean()}\\n    def loss_height(self,o,t,idx):lhc=sum([abs(ht[i].sum()-1)for ht,(i,_)in zip(o[:,:,2],idx)])/o.shape[0];return{'loss_height_constraint':lhc}\\n    def _get_src_p_idx(self,i):b=torch.cat([torch.full_like(s,k)for k,(s,_)in enumerate(i)]);s=torch.cat([s for(s,_)in i]);return b,s\\n    def get_loss(self,ln,o,t,i):return getattr(self,ln)(o,t,i)\\n    def forward(self,o,t):i=self.m(o,t);losses={};[losses.update(self.get_loss(ln,o,t,i)) for ln in self.l_names];return losses\\n\")\n",
        "with open(\"pretrain_autoencoder.py\", \"w\") as f: f.write(textwrap.dedent(\"\"\"\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "    from torch.nn import MSELoss\n",
        "    from ray import train\n",
        "    from ray.train.torch import TorchCheckpoint\n",
        "    from src.dataset_pretrain import AutoencoderDataset\n",
        "    from src.model import UNet\n",
        "\n",
        "    def train_loop_per_worker(config):\n",
        "        model = UNet(in_channels=config['in_channels'], activation=config['act_name'])\n",
        "        criterion = MSELoss()\n",
        "        optimizer_class = getattr(torch.optim, config['optimizer'])\n",
        "        optimizer = optimizer_class(model.parameters(), lr=config['lr'])\n",
        "        train_dataset = AutoencoderDataset(config)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=int(config['batch_size']))\n",
        "        model, train_dataloader = train.torch.prepare(model, train_dataloader)\n",
        "        for epoch in range(config['epochs']):\n",
        "            model.train(); running_loss = 0.0\n",
        "            for i, (image, _) in enumerate(train_dataloader):\n",
        "                outputs = model(image); loss = criterion(outputs, image)\n",
        "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "                running_loss += loss.item() * image.size(0)\n",
        "            epoch_loss = running_loss / len(train_dataset)\n",
        "            checkpoint = TorchCheckpoint.from_model(model=model.module)\n",
        "            train.report({\"loss\": epoch_loss}, checkpoint=checkpoint)\n",
        "    \"\"\"))\n",
        "with open(\"train_boundary_detector.py\", \"w\") as f: f.write(textwrap.dedent(\"\"\"\n",
        "    import torch, os\n",
        "    from torch.utils.data import DataLoader\n",
        "    from tqdm import tqdm\n",
        "    from src.dataset_finetune import BoundaryDataset\n",
        "    from src.model import W2WTransformerModel, load_pretrained_encoder_weights\n",
        "    from src.loss import SetCriterion\n",
        "    from src.utils import collate_fn\n",
        "\n",
        "    def run_finetuning(config):\n",
        "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); ft_params=config['finetuning']\n",
        "        loader=DataLoader(BoundaryDataset(config,seed=42), batch_size=ft_params['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "        model=W2WTransformerModel(config).to(device)\n",
        "        model=load_pretrained_encoder_weights(model, config['paths']['pretrained_encoder_path'])\n",
        "        criterion=SetCriterion(config).to(device); optimizer=torch.optim.AdamW(model.parameters(), lr=ft_params['learning_rate']); weight_dict=criterion.w\n",
        "        for epoch in range(ft_params['epochs']):\n",
        "            model.train(); total_loss=0; progress_bar=tqdm(loader, desc=f\"Epoch {epoch+1}/{ft_params['epochs']}\")\n",
        "            for images, targets in progress_bar:\n",
        "                images, targets = images.to(device), [{k:v.to(device) for k, v in t.items()} for t in targets]\n",
        "                outputs=model(images); loss_dict=criterion(outputs, targets); losses=sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "                optimizer.zero_grad(); losses.backward(); optimizer.step(); total_loss += losses.item()\n",
        "                progress_bar.set_postfix({'loss': f\"{losses.item():.4f}\"})\n",
        "            print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(loader):.4f}\")\n",
        "        os.makedirs(os.path.dirname(config['paths']['final_model_path']), exist_ok=True)\n",
        "        torch.save(model.state_dict(), config['paths']['final_model_path'])\n",
        "        print(f\"✅ Final model saved to {config['paths']['final_model_path']}\")\n",
        "    \"\"\"))\n",
        "with open(\"run_inference.py\", \"w\") as f: f.write(textwrap.dedent(\"\"\"\n",
        "    import pandas as pd, numpy as np, json, os\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.patches as patches\n",
        "    def plot_well_correlation(well1_name, well2_name, w1_layers, w2_layers, sim_matrix, threshold, output_path):\n",
        "        fig, ax = plt.subplots(figsize=(10, 12)); cmap = plt.get_cmap('viridis')\n",
        "        if not w1_layers or not w2_layers: print(\"Warning: One or both wells have no layers to plot.\"); return\n",
        "        max_depth = max(w1_layers[-1]['bottom'], w2_layers[-1]['bottom']) if w1_layers and w2_layers else 1000 # Default if layers are empty\n",
        "        ax.set_ylim(max_depth + 50, -50); ax.set_xlim(-0.5, 2.5)\n",
        "        for i, l in enumerate(w1_layers): ax.add_patch(patches.Rectangle((0, l['top']), 1, l['height'], edgecolor='black', facecolor=cmap(l.get('group_id', 0) / (len(w1_layers) if len(w1_layers)>0 else 1)), alpha=0.6))\n",
        "        for i, l in enumerate(w2_layers): ax.add_patch(patches.Rectangle((1.5, l['top']), 1, l['height'], edgecolor='black', facecolor=cmap(l.get('group_id', 0) / (len(w2_layers) if len(w2_layers)>0 else 1)), alpha=0.6))\n",
        "        for i, row in enumerate(sim_matrix):\n",
        "            for j, sim in enumerate(row):\n",
        "                if sim >= threshold:\n",
        "                    p = patches.Polygon([[1, w1_layers[i]['top']], [1, w1_layers[i]['bottom']], [1.5, w2_layers[j]['bottom']], [1.5, w2_layers[j]['top']]], facecolor=cmap(sim), alpha=0.4)\n",
        "                    ax.add_patch(p)\n",
        "        ax.set_xticks([0.5, 2]); ax.set_xticklabels([well1_name, well2_name], fontsize=14)\n",
        "        ax.set_ylabel(\"Depth\", fontsize=12); ax.set_title(\"Well to Well Correlation\", fontsize=16); plt.grid(True, axis='y', linestyle='--'); plt.savefig(output_path); plt.close()\n",
        "        print(f\"--> Correlation plot saved to {output_path}\")\n",
        "    def run_correlation(config):\n",
        "        inf, p = config['inference'], config['paths']; full_data = pd.read_csv(p['processed_csv_path'], delimiter=';'); ref_df = full_data[full_data['WELL'] == inf['reference_well']]; woi_df = full_data[full_data['WELL'] == inf['well_of_interest']]\n",
        "        if ref_df.empty or woi_df.empty: print(f\"Error: One or both wells not found: {inf['reference_well']}, {inf['well_of_interest']}. Check names in config.yaml or your data.\"); return\n",
        "        with open(p['label_encoder_path']) as f: label_encoder = json.load(f)\n",
        "        def get_true_layers(df, label_map):\n",
        "            df=df.copy().reset_index(drop=True); df['group_id']=df['GROUP'].astype(str).map(label_map).fillna(-1)\n",
        "            diffs=df['group_id'].diff().ne(0); layer_indices=np.concatenate(([0],df.index[diffs].values,[len(df)-1]))\n",
        "            layers=[{'top':df['DEPTH_MD'].iloc[layer_indices[i]],'bottom':df['DEPTH_MD'].iloc[layer_indices[i+1]-1],'height':df['DEPTH_MD'].iloc[layer_indices[i+1]-1]-df['DEPTH_MD'].iloc[layer_indices[i]],'group_id':df['group_id'].iloc[layer_indices[i]]} for i in range(len(layer_indices)-1) if layer_indices[i]<layer_indices[i+1]]\n",
        "            return layers\n",
        "        ref_layers=get_true_layers(ref_df,label_encoder); woi_layers=get_true_layers(woi_df,label_encoder)\n",
        "        sim_matrix=np.zeros((len(ref_layers),len(woi_layers)))\n",
        "        for i, l1 in enumerate(ref_layers):\n",
        "            for j, l2 in enumerate(woi_layers):\n",
        "                if l1.get('group_id')==l2.get('group_id') and l1.get('group_id')!=-1: sim_matrix[i,j]=np.random.uniform(0.8,0.95)\n",
        "                else: sim_matrix[i,j]=np.random.uniform(0.1,0.4)\n",
        "        print(\"--> MOCK INFERENCE: Using ground truth layers for visualization demonstration.\")\n",
        "        plot_well_correlation(inf['reference_well'],inf['well_of_interest'],ref_layers,woi_layers,sim_matrix,inf['correlation_threshold'],'well_correlation_plot.png')\n",
        "    \"\"\"))\n",
        "\n",
        "# THIS IS THE CORRECTED main.py SCRIPT\n",
        "with open(\"main.py\", \"w\") as f: f.write(textwrap.dedent(\"\"\"\n",
        "    import yaml, argparse, os, shutil, torch, mlflow\n",
        "    from ray import tune\n",
        "    from ray.train import RunConfig, ScalingConfig\n",
        "    from ray.train.torch import TorchTrainer\n",
        "    from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
        "    from src.prepare_data import run_data_preparation\n",
        "    from pretrain_autoencoder import train_loop_per_worker\n",
        "    from train_boundary_detector import run_finetuning\n",
        "    from run_inference import run_correlation\n",
        "\n",
        "    def main(config_path):\n",
        "        with open(config_path,'r') as file: config=yaml.safe_load(file)\n",
        "        mlflow.set_experiment(config['mlflow']['experiment_name'])\n",
        "\n",
        "        if config.get('run_data_preparation',False):\n",
        "            run_data_preparation(config)\n",
        "            print(\"\\\\n--- STAGE 0 COMPLETE ---\")\n",
        "\n",
        "        if config.get('run_pretraining',False):\n",
        "            if not os.path.exists(config['paths']['processed_csv_path']):\n",
        "                print(\"Error: 'processed_csv_path' not found. Run data prep first.\")\n",
        "                return\n",
        "            print(\"\\\\n--- LAUNCHING PIPELINE 1: AUTOENCODER PRE-TRAINING (VIA RAY TUNE) ---\")\n",
        "\n",
        "            # 1. Define the search space for hyperparameters that will be tuned.\n",
        "            param_space = config['pretraining']['search_space']\n",
        "\n",
        "            # 2. Define the static configuration that is the SAME for all trials.\n",
        "            train_loop_config = {\n",
        "                \"paths\": config[\"paths\"],\n",
        "                \"epochs\": config[\"pretraining\"][\"epochs\"]\n",
        "            }\n",
        "\n",
        "            # 3. Create the Tuner, which orchestrates the hyperparameter search.\n",
        "            tuner = tune.Tuner(\n",
        "                TorchTrainer(\n",
        "                    train_loop_per_worker,\n",
        "                    train_loop_config=train_loop_config,\n",
        "                    scaling_config=ScalingConfig(use_gpu=torch.cuda.is_available(), num_workers=1),\n",
        "                ),\n",
        "                param_space=param_space,\n",
        "                tune_config=tune.TuneConfig(\n",
        "                    num_samples=config['pretraining']['num_samples'],\n",
        "                    metric=\"loss\",\n",
        "                    mode=\"min\",\n",
        "                ),\n",
        "                run_config=RunConfig(\n",
        "                    name=\"Pre-training_Trial\",\n",
        "                    callbacks=[MLflowLoggerCallback(experiment_name=config['mlflow']['experiment_name'], save_artifact=True)],\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            results = tuner.fit()\n",
        "            best_result = results.get_best_result(metric=\"loss\", mode=\"min\")\n",
        "\n",
        "            if best_result and best_result.checkpoint:\n",
        "                source_path = os.path.join(best_result.checkpoint.path, \"model.pt\")\n",
        "                destination_path = config['paths']['pretrained_encoder_path']\n",
        "                os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
        "                shutil.copy(source_path, destination_path)\n",
        "                print(f\"\\\\n🏆 Best trial found with validation loss: {best_result.metrics['loss']:.4f}\")\n",
        "                print(f\"✅ Best pre-trained model saved to {destination_path}\")\n",
        "            else:\n",
        "                print(\"⚠️ No best trial found or best trial had no checkpoint. Pre-training may have failed.\")\n",
        "            print(\"\\\\n--- STAGE 1 COMPLETE ---\")\n",
        "\n",
        "        if config.get('run_finetuning',False):\n",
        "            if not os.path.exists(config['paths']['pretrained_encoder_path']):\n",
        "                print(\"Error: 'pretrained_encoder_path' not found. Run pre-training first.\")\n",
        "                return\n",
        "            with mlflow.start_run(run_name=\"Fine-tuning_Run\") as run:\n",
        "                print(f\"\\\\n--- LAUNCHING PIPELINE 2: FINE-TUNING (MLflow Run ID: {run.info.run_id}) ---\")\n",
        "                mlflow.log_params(config['finetuning'])\n",
        "                run_finetuning(config)\n",
        "                mlflow.log_artifact(config['paths']['final_model_path'])\n",
        "                print(\"\\\\n--- STAGE 2 COMPLETE ---\")\n",
        "\n",
        "        if config.get('run_inference',False):\n",
        "            if not os.path.exists(config['paths']['final_model_path']):\n",
        "                print(\"Error: 'final_model_path' not found. Run fine-tuning first.\")\n",
        "                return\n",
        "            with mlflow.start_run(run_name=\"Inference_Correlation_Run\") as run:\n",
        "                print(f\"\\\\n--- LAUNCHING PIPELINE 3: WELL-TO-WELL INFERENCE (MLflow Run ID: {run.info.run_id}) ---\")\n",
        "                mlflow.log_params(config['inference'])\n",
        "                run_correlation(config)\n",
        "                mlflow.log_artifact('well_correlation_plot.png')\n",
        "                print(\"\\\\n--- STAGE 3 COMPLETE ---\")\n",
        "        print(\"\\\\n✅ All requested pipeline stages finished.\")\n",
        "\n",
        "    if __name__==\"__main__\":\n",
        "        parser=argparse.ArgumentParser()\n",
        "        parser.add_argument('--config', type=str, default='config.yaml')\n",
        "        args, unknown = parser.parse_known_args()\n",
        "        main(args.config)\n",
        "    \"\"\"))\n",
        "\n",
        "print(\"\\n✅ All project files created successfully!\")"
      ],
      "metadata": {
        "id": "q-eIrLMeWY8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d916006-4fe6-48b5-b912-d19fa9b8e49b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Creating all project source files...\n",
            "\n",
            "✅ All project files created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After running the previous cell, execute this cell and then\n",
        "# GO TO \"Runtime\" -> \"Restart runtime...\" in the Colab menu.\n",
        "# Confirm the restart, and then proceed to the next cell.\n",
        "\n",
        "print(\"!!! IMPORTANT: Please RESTART YOUR COLAB RUNTIME NOW !!!\")\n",
        "print(\"Go to 'Runtime' -> 'Restart runtime...' in the Colab menu.\")\n",
        "print(\"Once restarted, continue to the next cell.\")"
      ],
      "metadata": {
        "id": "pryGy5VpWY-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dab1370-2dd4-47e9-9f62-ee57db7bb62d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! IMPORTANT: Please RESTART YOUR COLAB RUNTIME NOW !!!\n",
            "Go to 'Runtime' -> 'Restart runtime...' in the Colab menu.\n",
            "Once restarted, continue to the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\">>> ACTION REQUIRED: Please upload the ZIP file containing your .las files.\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "if not uploaded_files:\n",
        "    print(\"\\n⚠️ Upload was cancelled or failed. Please run this cell again.\")\n",
        "elif len(uploaded_files) > 1:\n",
        "    print(\"\\n⚠️ Please upload only a single ZIP file. Run this cell again.\")\n",
        "else:\n",
        "    zip_filename = list(uploaded_files.keys())[0]\n",
        "    print(f\"\\n✅ '{zip_filename}' uploaded successfully.\")\n",
        "\n",
        "    print(\"--> Unzipping into the 'data/raw_las_files' folder...\")\n",
        "    # Use -o to overwrite existing files without prompting\n",
        "    !unzip -q -o \"{zip_filename}\" -d data/raw_las_files/\n",
        "\n",
        "    print(\"--> ZIP file has been unzipped successfully.\")\n",
        "\n",
        "    # Clean up the uploaded zip file\n",
        "    os.remove(zip_filename)\n",
        "    print(\"\\n✅ Data input step is complete. You can now proceed to the next cell.\")"
      ],
      "metadata": {
        "id": "wJuD2s45WZBR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "c4ff2f1b-131e-456b-d325-5d5e3fa988f2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> ACTION REQUIRED: Please upload the ZIP file containing your .las files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e009bea5-6aea-4937-bef4-025ee3885d49\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e009bea5-6aea-4937-bef4-025ee3885d49\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.zip to train.zip\n",
            "\n",
            "✅ 'train.zip' uploaded successfully.\n",
            "--> Unzipping into the 'data/raw_las_files' folder...\n",
            "--> ZIP file has been unzipped successfully.\n",
            "\n",
            "✅ Data input step is complete. You can now proceed to the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ⚙️ Configure Your Pipeline Run\n",
        "#@markdown ### 1. Select Which Pipelines to Run\n",
        "#@markdown Check the boxes for all stages you want to execute in this session.\n",
        "run_data_preparation = True #@param {type:\"boolean\"}\n",
        "run_pretraining = True #@param {type:\"boolean\"}\n",
        "run_finetuning = True #@param {type:\"boolean\"}\n",
        "run_inference = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Configure Model and Data Parameters\n",
        "#@markdown **Important:** Set the number of feature columns (curves) from your LAS files.\n",
        "input_channels = 13 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **For the final Inference stage**, provide the exact names of the wells to compare.\n",
        "#@markdown (You can find these in 'data/train.csv' after running data preparation).\n",
        "reference_well = \"WELL_NAME_A\" #@param {type:\"string\"}\n",
        "well_of_interest = \"WELL_NAME_B\" #@param {type:\"string\"}\n",
        "\n",
        "# --- DO NOT EDIT THE CODE BELOW ---\n",
        "import yaml\n",
        "\n",
        "print(\"--> Reading existing config.yaml file...\")\n",
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"--> Applying your configuration settings...\")\n",
        "\n",
        "config['run_data_preparation'] = run_data_preparation\n",
        "config['run_pretraining'] = run_pretraining\n",
        "config['run_finetuning'] = run_finetuning\n",
        "config['run_inference'] = run_inference\n",
        "\n",
        "# Correctly place in_channels inside the search_space for pretraining and model_params for finetuning\n",
        "config['pretraining']['search_space']['in_channels'] = input_channels\n",
        "config['finetuning']['model_params']['in_channels'] = input_channels\n",
        "config['inference']['reference_well'] = reference_well\n",
        "config['inference']['well_of_interest'] = well_of_interest\n",
        "\n",
        "with open('config.yaml', 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "print(\"--> Successfully updated config.yaml with your settings.\")\n",
        "print(\"\\n✅ Configuration complete. You are ready to run the main pipeline.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQw3OAigMi1N",
        "outputId": "9a57c034-b08d-4e25-cd66-1571f7d167d5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Reading existing config.yaml file...\n",
            "--> Applying your configuration settings...\n",
            "--> Successfully updated config.yaml with your settings.\n",
            "\n",
            "✅ Configuration complete. You are ready to run the main pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🚀 Run Pipeline & Launch MLflow UI\n",
        "#@markdown 1. **(One-time setup)** Go to https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "#@markdown 2. Copy your authtoken and paste it below.\n",
        "#@markdown 3. Run this cell to execute the pipeline and view the results.\n",
        "\n",
        "ngrok_auth_token = \"2zsOgKyrXbZYrtq5ojgMgwih7AQ_4QwzvhaUh1PL7MYgYB9nY\" #@param {type:\"string\"}\n",
        "\n",
        "# --- DO NOT EDIT THE CODE BELOW ---\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Authenticate ngrok\n",
        "if \"PASTE\" not in ngrok_auth_token and ngrok_auth_token:\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "    print(\"✅ Ngrok token set successfully.\")\n",
        "else:\n",
        "    print(\"⚠️ Ngrok token not set. UI will not launch. Please get a token from ngrok.com\")\n",
        "\n",
        "# Terminate previous ngrok/mlflow processes to ensure a clean start\n",
        "print(\"--> Cleaning up previous processes...\")\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except Exception:\n",
        "    pass # No running tunnels to kill\n",
        "\n",
        "# Find and kill any lingering mlflow processes\n",
        "# Using subprocess.run for more control and error handling\n",
        "try:\n",
        "    # List all processes and filter for 'mlflow ui'\n",
        "    output = subprocess.run(['ps', '-ax'], capture_output=True, text=True, check=True).stdout\n",
        "    for line in output.splitlines():\n",
        "        if 'mlflow ui' in line and 'grep' not in line:\n",
        "            try:\n",
        "                pid = int(line.split()[0])\n",
        "                os.kill(pid, signal.SIGTERM) # Use SIGTERM for graceful shutdown\n",
        "                print(f\"    - Sent SIGTERM to lingering mlflow process (PID: {pid})\")\n",
        "                time.sleep(1) # Give it a moment to terminate\n",
        "                # Check if process is still alive, if so, send SIGKILL\n",
        "                if os.path.exists(f\"/proc/{pid}\"): # Linux-specific check\n",
        "                    os.kill(pid, signal.SIGKILL)\n",
        "                    print(f\"    - Killed lingering mlflow process (PID: {pid}) with SIGKILL\")\n",
        "            except (ValueError, ProcessLookupError):\n",
        "                pass # Process might have already died or PID parsing failed\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not clean up old MLflow processes. Error: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- RUNNING THE MAIN PIPELINE SCRIPT ---\")\n",
        "# --- Run the main script ---\n",
        "!python main.py\n",
        "print(\"--- PIPELINE SCRIPT FINISHED ---\")\n",
        "\n",
        "\n",
        "# --- Launch the MLflow UI ---\n",
        "print(\"\\n--> Launching MLflow UI...\")\n",
        "\n",
        "# Start MLflow UI in the background\n",
        "# Using subprocess.Popen for more robust background execution\n",
        "mlflow_process = None\n",
        "try:\n",
        "    mlflow_process = subprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", \"mlruns/\", \"--port\", \"5000\"],\n",
        "                                      stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid)\n",
        "    print(\"MLflow UI process started in background.\")\n",
        "    time.sleep(5) # Give MLflow UI some time to start up\n",
        "\n",
        "    # Create a public URL to the MLflow UI\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(f\"✅ MLflow UI is running. Click here: {public_url}\")\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"🚨 Error launching MLflow UI or Ngrok tunnel: {e}\")\n",
        "    if mlflow_process:\n",
        "        print(\"MLflow process output (stdout):\", mlflow_process.stdout.read().decode())\n",
        "        print(\"MLflow process output (stderr):\", mlflow_process.stderr.read().decode())\n",
        "\n",
        "print(\"\\n--- MLflow UI setup complete. You can now access the UI via the link above. ---\")\n",
        "print(\"Note: The MLflow UI will remain active as long as this Colab session is running or until you kill the process.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dzWIpu5dTZX",
        "outputId": "3292f211-e38c-41c3-9c39-6847f97e3f4e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Ngrok token set successfully.\n",
            "--> Cleaning up previous processes...\n",
            "    - Sent SIGTERM to lingering mlflow process (PID: 4883)\n",
            "\n",
            "--- RUNNING THE MAIN PIPELINE SCRIPT ---\n",
            "--- LAUNCHING PIPELINE 0: DATA PREPARATION ---\n",
            "--> Found single sub-folder 'Force_2020_all_wells_train_test_blind_hidden_final'. Adjusting search path.\n",
            "--> Searching for .las files in 'data/raw_las_files/Force_2020_all_wells_train_test_blind_hidden_final'...\n",
            "--> Found 118 .las files. Reading now...\n",
            "--> Saved combined data to 'data/train.csv'\n",
            "--> Saved label encoder to 'artifacts/label_encoder.json'\n",
            "--> Saved StandardScaler to 'artifacts/StandardScaler.bin'\n",
            "✅ Data Preparation complete.\n",
            "\n",
            "--- STAGE 0 COMPLETE ---\n",
            "\n",
            "--- LAUNCHING PIPELINE 1: AUTOENCODER PRE-TRAINING (VIA RAY TUNE) ---\n",
            "2025-07-16 18:40:54,122\tINFO worker.py:1917 -- Started a local Ray instance.\n",
            "2025-07-16 18:40:57,509\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "╭────────────────────────────────────────────────────────╮\n",
            "│ Configuration for experiment     Pre-training_Trial    │\n",
            "├────────────────────────────────────────────────────────┤\n",
            "│ Search algorithm                 BasicVariantGenerator │\n",
            "│ Scheduler                        FIFOScheduler         │\n",
            "│ Number of trials                 10                    │\n",
            "╰────────────────────────────────────────────────────────╯\n",
            "\n",
            "View detailed results here: /root/ray_results/Pre-training_Trial\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts`\n",
            "\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2025-07-16 18:41:00. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "╭─────────────────────────────────────╮\n",
            "│ Trial name                 status   │\n",
            "├─────────────────────────────────────┤\n",
            "│ TorchTrainer_69907_00000   PENDING  │\n",
            "│ TorchTrainer_69907_00001   PENDING  │\n",
            "│ TorchTrainer_69907_00002   PENDING  │\n",
            "│ TorchTrainer_69907_00003   PENDING  │\n",
            "│ TorchTrainer_69907_00004   PENDING  │\n",
            "│ TorchTrainer_69907_00005   PENDING  │\n",
            "│ TorchTrainer_69907_00006   PENDING  │\n",
            "│ TorchTrainer_69907_00007   PENDING  │\n",
            "│ TorchTrainer_69907_00008   PENDING  │\n",
            "│ TorchTrainer_69907_00009   PENDING  │\n",
            "╰─────────────────────────────────────╯\n",
            "\n",
            "Trial TorchTrainer_69907_00001 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_69907_00001 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "\n",
            "Trial TorchTrainer_69907_00000 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_69907_00000 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-16 18:41:08,331\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_69907_00000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=51007, ip=172.28.0.12, actor_id=17d1943b1b2ee007733a1cf101000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_69907_00000 errored after 0 iterations at 2025-07-16 18:41:08. Total running time: 8s\n",
            "Error file: /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts/TorchTrainer_69907_00000_0_2025-07-16_18-40-59/error.txt\n",
            "2025-07-16 18:41:08,461\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_69907_00001\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=51006, ip=172.28.0.12, actor_id=2d2f9731efaad5c5a956b8ef01000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_69907_00001 errored after 0 iterations at 2025-07-16 18:41:08. Total running time: 8s\n",
            "Error file: /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts/TorchTrainer_69907_00001_1_2025-07-16_18-40-59/error.txt\n",
            "\n",
            "Trial TorchTrainer_69907_00003 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_69907_00003 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "\n",
            "Trial TorchTrainer_69907_00002 started with configuration:\n",
            "╭────────────────────────────────────────────────────────────────╮\n",
            "│ Trial TorchTrainer_69907_00002 config                          │\n",
            "├────────────────────────────────────────────────────────────────┤\n",
            "│ act_name                                     ['prelu', 'relu'] │\n",
            "│ batch_size                                            [16, 32] │\n",
            "│ in_channels                                                 13 │\n",
            "│ lr                                             [0.001, 0.0001] │\n",
            "│ optimizer                                 ... 'AdamW', 'Adam'] │\n",
            "╰────────────────────────────────────────────────────────────────╯\n",
            "2025-07-16 18:41:18,856\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_69907_00003\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=51200, ip=172.28.0.12, actor_id=7d90511f6f0ef5ed02e04cd501000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_69907_00003 errored after 0 iterations at 2025-07-16 18:41:18. Total running time: 18s\n",
            "Error file: /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts/TorchTrainer_69907_00003_3_2025-07-16_18-41-00/error.txt\n",
            "2025-07-16 18:41:18,880\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_69907_00002\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2849, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 937, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=51198, ip=172.28.0.12, actor_id=f4646e38c252798cb716791301000000, repr=TorchTrainer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 883, in _trainable_func\n",
            "    super()._trainable_func(self._merged_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/train/base_trainer.py\", line 106, in _train_coordinator_fn\n",
            "    trainer = trainer_cls(**config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TorchTrainer.__init__() got an unexpected keyword argument 'in_channels'\n",
            "\n",
            "Trial TorchTrainer_69907_00002 errored after 0 iterations at 2025-07-16 18:41:18. Total running time: 18s\n",
            "Error file: /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts/TorchTrainer_69907_00002_2_2025-07-16_18-40-59/error.txt\n",
            "2025-07-16 18:41:29,757\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2025-07-16 18:41:29,765\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/Pre-training_Trial' in 0.0079s.\n",
            "\n",
            "Trial status: 4 ERROR | 6 PENDING\n",
            "Current time: 2025-07-16 18:41:29. Total running time: 29s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "╭─────────────────────────────────────╮\n",
            "│ Trial name                 status   │\n",
            "├─────────────────────────────────────┤\n",
            "│ TorchTrainer_69907_00004   PENDING  │\n",
            "│ TorchTrainer_69907_00005   PENDING  │\n",
            "│ TorchTrainer_69907_00006   PENDING  │\n",
            "│ TorchTrainer_69907_00007   PENDING  │\n",
            "│ TorchTrainer_69907_00008   PENDING  │\n",
            "│ TorchTrainer_69907_00009   PENDING  │\n",
            "│ TorchTrainer_69907_00000   ERROR    │\n",
            "│ TorchTrainer_69907_00001   ERROR    │\n",
            "│ TorchTrainer_69907_00002   ERROR    │\n",
            "│ TorchTrainer_69907_00003   ERROR    │\n",
            "╰─────────────────────────────────────╯\n",
            "\n",
            "Number of errored trials: 4\n",
            "╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
            "│ Trial name                   # failures   error file                                                                                                                                                                   │\n",
            "├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ TorchTrainer_69907_00000              1   /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts/TorchTrainer_69907_00000_0_2025-07-16_18-40-59/error.txt │\n",
            "│ TorchTrainer_69907_00001              1   /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts/TorchTrainer_69907_00001_1_2025-07-16_18-40-59/error.txt │\n",
            "│ TorchTrainer_69907_00002              1   /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts/TorchTrainer_69907_00002_2_2025-07-16_18-40-59/error.txt │\n",
            "│ TorchTrainer_69907_00003              1   /tmp/ray/session_2025-07-16_18-40-51_679455_50107/artifacts/2025-07-16_18-40-57/Pre-training_Trial/driver_artifacts/TorchTrainer_69907_00003_3_2025-07-16_18-41-00/error.txt │\n",
            "╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
            "\n",
            "2025-07-16 18:41:33,254\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_69907_00000, TorchTrainer_69907_00001, TorchTrainer_69907_00002, TorchTrainer_69907_00003]\n",
            "2025-07-16 18:41:33,256\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
            "Resume experiment with: Tuner.restore(path=\"/root/ray_results/Pre-training_Trial\", trainable=...)\n",
            "2025-07-16 18:41:33,267\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 6 trial(s):\n",
            "- TorchTrainer_69907_00004: FileNotFoundError('Could not fetch metrics for TorchTrainer_69907_00004: both result.json and progress.csv were not found at /root/ray_results/Pre-training_Trial/TorchTrainer_69907_00004_4_2025-07-16_18-41-00')\n",
            "- TorchTrainer_69907_00005: FileNotFoundError('Could not fetch metrics for TorchTrainer_69907_00005: both result.json and progress.csv were not found at /root/ray_results/Pre-training_Trial/TorchTrainer_69907_00005_5_2025-07-16_18-41-00')\n",
            "- TorchTrainer_69907_00006: FileNotFoundError('Could not fetch metrics for TorchTrainer_69907_00006: both result.json and progress.csv were not found at /root/ray_results/Pre-training_Trial/TorchTrainer_69907_00006_6_2025-07-16_18-41-00')\n",
            "- TorchTrainer_69907_00007: FileNotFoundError('Could not fetch metrics for TorchTrainer_69907_00007: both result.json and progress.csv were not found at /root/ray_results/Pre-training_Trial/TorchTrainer_69907_00007_7_2025-07-16_18-41-00')\n",
            "- TorchTrainer_69907_00008: FileNotFoundError('Could not fetch metrics for TorchTrainer_69907_00008: both result.json and progress.csv were not found at /root/ray_results/Pre-training_Trial/TorchTrainer_69907_00008_8_2025-07-16_18-41-00')\n",
            "- TorchTrainer_69907_00009: FileNotFoundError('Could not fetch metrics for TorchTrainer_69907_00009: both result.json and progress.csv were not found at /root/ray_results/Pre-training_Trial/TorchTrainer_69907_00009_9_2025-07-16_18-41-00')\n",
            "2025-07-16 18:41:33,270\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/W2W_Pipeline_Local/main.py\", line 95, in <module>\n",
            "    main(args.config)\n",
            "  File \"/content/W2W_Pipeline_Local/main.py\", line 55, in main\n",
            "    best_result = results.get_best_result(metric=\"loss\", mode=\"min\")\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/result_grid.py\", line 161, in get_best_result\n",
            "    raise RuntimeError(error_msg)\n",
            "RuntimeError: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.\n",
            "\u001b[0m--- PIPELINE SCRIPT FINISHED ---\n",
            "\n",
            "--> Launching MLflow UI...\n",
            "MLflow UI process started in background.\n",
            "---------------------------------------------------------------\n",
            "✅ MLflow UI is running. Click here: NgrokTunnel: \"https://c2e276fcd2d5.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "---------------------------------------------------------------\n",
            "\n",
            "--- MLflow UI setup complete. You can now access the UI via the link above. ---\n",
            "Note: The MLflow UI will remain active as long as this Colab session is running or until you kill the process.\n"
          ]
        }
      ]
    }
  ]
}